{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ix5dQS2rUMlu"
   },
   "source": [
    "#EECS 504 PS4: Backpropagation\n",
    "\n",
    "Please provide the following information \n",
    "(e.g. Andrew Owens, ahowens):\n",
    "\n",
    "[Aohan] [Mei], [rogermei]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_Cst4k4tuBc"
   },
   "source": [
    "# Starting\n",
    "\n",
    "Run the following code to import the modules you'll need. After your finish the assignment, remember to run all cells and save the note book to your local machine as a .ipynb file for Canvas submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100,
     "referenced_widgets": [
      "ad34ee2892d94575b52b07b62e23d836",
      "c7fdbf1467c247f5b7b4bb58fc705c40",
      "75890ba2430a4066add67456e197a51c",
      "634f9f36e3234760a8b6232c057eb84b",
      "c423ef0139c7460bb1aa6c5e6e19956e",
      "07bdddedb4ff4979ac8528ca64fedef9",
      "ddbd8ffe4d6d4ab29ad5aed91c0c1c63",
      "8d4d0dbbd0db49ddadc4561f6e5f3188"
     ]
    },
    "colab_type": "code",
    "id": "SHumIO-xt57H",
    "outputId": "29268c7d-7325-47af-8f38-eedf8fbf0f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad34ee2892d94575b52b07b62e23d836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./cifar-10-python.tar.gz to .\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torchvision.datasets import CIFAR10\n",
    "download = not os.path.isdir('cifar-10-batches-py')\n",
    "dset_train = CIFAR10(root='.', download=download)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apEPzDNtK0MC"
   },
   "source": [
    "# Problem 4.2 Multi-layer perceptron\n",
    "In this problem you will develop a two Layer neural network with fully-connected layers to perform classification, and test it out on the CIFAR-10 dataset.\n",
    "\n",
    "We train the network with a softmax loss function on the weight matrices. The network uses a ReLU nonlinearity after the first fully connected layer. In other words, the network has the following architecture:\n",
    "\n",
    "input - fully connected layer - ReLU - fully connected layer - softmax\n",
    "\n",
    "The outputs of the second fully-connected layer are the scores for each class.\n",
    "\n",
    "You cannot use any deep learning libraries such as PyTorch in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXfumCQ21JoK"
   },
   "source": [
    "# 4.2 (a) Layers\n",
    "In this problem, implement fully connected layer, relu and softmax. Filling in all TODOs in skeleton codes will be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q-ljfgMv9PHx"
   },
   "outputs": [],
   "source": [
    "def fc_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a fully-connected layer.\n",
    "    \n",
    "    The input x has shape (N, Din) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (Din,).\n",
    "    \n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, Din)\n",
    "    - w: A numpy array of weights, of shape (Din, Dout)\n",
    "    - b: A numpy array of biases, of shape (Dout,)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, Dout)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the forward pass. Store the result in out.              #\n",
    "    ###########################################################################\n",
    "    out = np.matmul(x,w) + b\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = (x, w, b)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def fc_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a fully_connected layer.\n",
    "    \n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, Dout)\n",
    "    - cache: returned by your forward function. Tuple of:\n",
    "      - x: Input data, of shape (N, Din)\n",
    "      - w: Weights, of shape (Din, Dout)\n",
    "      - b: Biases, of shape (Dout,)\n",
    "      \n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, Din)\n",
    "    - dw: Gradient with respect to w, of shape (Din, Dout)\n",
    "    - db: Gradient with respect to b, of shape (Dout,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the affine backward pass.                               #\n",
    "    ###########################################################################\n",
    "    dx = np.matmul(dout,np.transpose(w))\n",
    "    dw = np.matmul(np.transpose(x),dout)\n",
    "    db = np.sum(dout,axis=0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx, dw, db\n",
    "\n",
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = x\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU forward pass.                                  #\n",
    "    ###########################################################################\n",
    "    out = np.maximum(x,0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    cache = x\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: returned by your forward function. Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = dout, cache\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the ReLU backward pass.                                 #\n",
    "    ###########################################################################\n",
    "    dx[x<0] = 0\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return dx\n",
    "\n",
    "\n",
    "def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss, dx = None, None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement softmax loss                                            #\n",
    "    ###########################################################################\n",
    "    x_demax = x - np.max(x,axis=1,keepdims=True)\n",
    "    N = np.shape(y)[0]\n",
    "    prob = np.exp(x_demax)/np.sum(np.exp(x_demax),axis=1,keepdims=True)\n",
    "    loss = -np.sum(np.log(prob[np.arange(N),y]))/N\n",
    "    prob[np.arange(N),y] = prob[np.arange(N),y] - 1.\n",
    "    dx = prob\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbFxtS3zK8oz"
   },
   "source": [
    "# 4.2 (b) Softmax Classifier\n",
    "\n",
    "In this problem, implement softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytvxbx9UpxVL"
   },
   "outputs": [],
   "source": [
    "class SoftmaxClassifier(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with\n",
    "    softmax loss that uses a modular layer design. We assume an input dimension\n",
    "    of D, a hidden dimension of H, and perform classification over C classes.\n",
    "\n",
    "    The architecture should be fc - relu - fc - softmax with one hidden layer\n",
    "\n",
    "    The learnable parameters of the model are stored in the dictionary\n",
    "    self.params that maps parameter names to numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=3072, hidden_dim=300, num_classes=10,\n",
    "                 weight_scale=1e-3):\n",
    "        \"\"\"\n",
    "        Initialize a new network.\n",
    "\n",
    "        Inputs:\n",
    "        - input_dim: An integer giving the size of the input\n",
    "        - hidden_dim: An integer giving the size of the hidden layer, None\n",
    "          if there's no hidden layer.\n",
    "        - num_classes: An integer giving the number of classes to classify\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        \"\"\"\n",
    "        self.params = {}\n",
    "        ############################################################################\n",
    "        # TODO: Initialize the weights and biases of the two-layer net. Weights    #\n",
    "        # should be initialized from a Gaussian centered at 0.0 with               #\n",
    "        # standard deviation equal to weight_scale, and biases should be           #\n",
    "        # initialized to zero. All weights and biases should be stored in the      #\n",
    "        # dictionary self.params, with fc weights and biases using the keys        #\n",
    "        # 'W' and 'b', i.e., W1, b1 for the weights and bias in the first linear   #\n",
    "        # layer, W2, b2 for the weights and bias in the second linear layer.       #\n",
    "        ############################################################################\n",
    "        self.params[\"W1\"] = np.random.normal(0,weight_scale,(input_dim,hidden_dim))\n",
    "        self.params[\"b1\"] = np.zeros(hidden_dim)\n",
    "        self.params[\"W2\"] = np.random.normal(0,weight_scale,(hidden_dim,num_classes))\n",
    "        self.params[\"b2\"] = np.zeros(num_classes)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "\n",
    "\n",
    "    def forwards_backwards(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for a minibatch of data.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, Din)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "          scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass. And\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "          names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        scores = None\n",
    "        ############################################################################\n",
    "        # TODO: Implement the forward pass for the two-layer net, computing the    #\n",
    "        # class scores for X and storing them in the scores variable.              #\n",
    "        ############################################################################\n",
    "        fc_layer1_out, fc_layer1_cache = fc_forward(X, self.params[\"W1\"], self.params[\"b1\"])\n",
    "        rl_layer1_out, rl_layer1_cache = relu_forward(fc_layer1_out)\n",
    "        scores, fc_layer2_cache = fc_forward(rl_layer1_out,self.params[\"W2\"], self.params[\"b2\"])\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        # If y is None then we are in test mode so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0, {}\n",
    "        ############################################################################\n",
    "        # TODO: Implement the backward pass for the two-layer net. Store the loss  #\n",
    "        # in the loss variable and gradients in the grads dictionary. Compute data #\n",
    "        # loss using softmax, and make sure that grads[k] holds the gradients for  #\n",
    "        # self.params[k].                                                          # \n",
    "        ############################################################################\n",
    "        loss, dx_softmax = softmax_loss(scores,y)\n",
    "        dx_fc_layer2, grads[\"W2\"], grads[\"b2\"] = fc_backward(dx_softmax,fc_layer2_cache)\n",
    "        dx_relu_layer1 = relu_backward(dx_fc_layer2,rl_layer1_cache)\n",
    "        dx_fc_layer1, grads[\"W1\"], grads[\"b1\"] = fc_backward(dx_relu_layer1,fc_layer1_cache)\n",
    "        ############################################################################\n",
    "        #                             END OF YOUR CODE                             #\n",
    "        ############################################################################\n",
    "        return loss, grads\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lwp0waIL1h_e"
   },
   "source": [
    "# 4.2(c) Training\n",
    "\n",
    "In this problem, you need to preprocess the images and set up model hyperparameters. Notice that adjust the training and val split is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "kZPtQzXGMoCg",
    "outputId": "0c6336a2-fe24-4bc8-ba0b-dd274f5cd724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 6250) loss: 2.314778\n",
      "(Epoch 0 / 10) train acc: 0.168000; val_acc: 0.170300\n",
      "(Epoch 1 / 10) train acc: 0.500000; val_acc: 0.437200\n",
      "(Iteration 1001 / 6250) loss: 1.366744\n",
      "(Epoch 2 / 10) train acc: 0.545000; val_acc: 0.481600\n",
      "(Epoch 3 / 10) train acc: 0.594000; val_acc: 0.477300\n",
      "(Iteration 2001 / 6250) loss: 1.279976\n",
      "(Epoch 4 / 10) train acc: 0.605000; val_acc: 0.486000\n",
      "(Iteration 3001 / 6250) loss: 1.025714\n",
      "(Epoch 5 / 10) train acc: 0.633000; val_acc: 0.467900\n",
      "(Epoch 6 / 10) train acc: 0.637000; val_acc: 0.463500\n",
      "(Iteration 4001 / 6250) loss: 0.699311\n",
      "(Epoch 7 / 10) train acc: 0.690000; val_acc: 0.481000\n",
      "(Epoch 8 / 10) train acc: 0.734000; val_acc: 0.494200\n",
      "(Iteration 5001 / 6250) loss: 0.938436\n",
      "(Epoch 9 / 10) train acc: 0.749000; val_acc: 0.493200\n",
      "(Iteration 6001 / 6250) loss: 1.156156\n",
      "(Epoch 10 / 10) train acc: 0.749000; val_acc: 0.497300\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding=\"latin1\")\n",
    "    return dict\n",
    "\n",
    "def load_cifar10():\n",
    "    data = {}\n",
    "    meta = unpickle(\"cifar-10-batches-py/batches.meta\")\n",
    "    batch1 = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "    batch2 = unpickle(\"cifar-10-batches-py/data_batch_2\")\n",
    "    batch3 = unpickle(\"cifar-10-batches-py/data_batch_3\")\n",
    "    batch4 = unpickle(\"cifar-10-batches-py/data_batch_4\")\n",
    "    batch5 = unpickle(\"cifar-10-batches-py/data_batch_5\")\n",
    "    test_batch = unpickle(\"cifar-10-batches-py/test_batch\")\n",
    "    X_train = np.vstack((batch1['data'], batch2['data'], batch3['data'],\\\n",
    "                         batch4['data'], batch5['data']))\n",
    "    Y_train = np.array(batch1['labels'] + batch2['labels'] + batch3['labels'] + \n",
    "                       batch4['labels'] + batch5['labels'])\n",
    "    X_test = test_batch['data']\n",
    "    Y_test = test_batch['labels']\n",
    "    \n",
    "    #Preprocess images here                                     \n",
    "    X_train = (X_train-np.mean(X_train,axis=1,keepdims=True))/np.std(X_train,axis=1,keepdims=True)\n",
    "    X_test = (X_test-np.mean(X_test,axis=1,keepdims=True))/np.std(X_test,axis=1,keepdims=True)\n",
    "\n",
    "    data['X_train'] = X_train[:40000]\n",
    "    data['y_train'] = Y_train[:40000]\n",
    "    data['X_val'] = X_train[40000:]\n",
    "    data['y_val'] = Y_train[40000:]\n",
    "    data['X_test'] = X_test\n",
    "    data['y_test'] = Y_test\n",
    "    return data\n",
    "\n",
    "def test_network(model, X, y, num_samples=None, batch_size=100):\n",
    "    \"\"\"\n",
    "    Check accuracy of the model on the provided data.\n",
    "\n",
    "    Inputs:\n",
    "    - model: Image classifier\n",
    "    - X: Array of data, of shape (N, d_1, ..., d_k)\n",
    "    - y: Array of labels, of shape (N,)\n",
    "    - num_samples: If not None, subsample the data and only test the model\n",
    "      on num_samples datapoints.\n",
    "    - batch_size: Split X and y into batches of this size to avoid using\n",
    "      too much memory.\n",
    "\n",
    "    Returns:\n",
    "    - acc: Scalar giving the fraction of instances that were correctly\n",
    "      classified by the model.\n",
    "    \"\"\"\n",
    "    # Subsample the data\n",
    "    N = X.shape[0]\n",
    "    if num_samples is not None and N > num_samples:\n",
    "        mask = np.random.choice(N, num_samples)\n",
    "        N = num_samples\n",
    "        X = X[mask]\n",
    "        y = y[mask]\n",
    "\n",
    "    # Compute predictions in batches\n",
    "    num_batches = N // batch_size\n",
    "    if N % batch_size != 0:\n",
    "        num_batches += 1\n",
    "    y_pred = []\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        scores = model.forwards_backwards(X[start:end])\n",
    "        y_pred.append(np.argmax(scores, axis=1))\n",
    "    y_pred = np.hstack(y_pred)\n",
    "    acc = np.mean(y_pred == y)\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_network(model, data, **kwargs):\n",
    "    \"\"\"\n",
    "     Required arguments:\n",
    "    - model: Image classifier\n",
    "    - data: A dictionary of training and validation data containing:\n",
    "      'X_train': Array, shape (N_train, d_1, ..., d_k) of training images\n",
    "      'X_val': Array, shape (N_val, d_1, ..., d_k) of validation images\n",
    "      'y_train': Array, shape (N_train,) of labels for training images\n",
    "      'y_val': Array, shape (N_val,) of labels for validation images\n",
    "\n",
    "    Optional arguments:\n",
    "    - learning_rate: A scalar for initial learning rate.\n",
    "    - lr_decay: A scalar for learning rate decay; after each epoch the\n",
    "      learning rate is multiplied by this value.\n",
    "    - batch_size: Size of minibatches used to compute loss and gradient\n",
    "      during training.\n",
    "    - num_epochs: The number of epochs to run for during training.\n",
    "    - print_every: Integer; training losses will be printed every\n",
    "      print_every iterations.\n",
    "    - verbose: Boolean; if set to false then no output will be printed\n",
    "      during training.\n",
    "    - num_train_samples: Number of training samples used to check training\n",
    "      accuracy; default is 1000; set to None to use entire training set.\n",
    "    - num_val_samples: Number of validation samples to use to check val\n",
    "      accuracy; default is None, which uses the entire validation set.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    learning_rate =  kwargs.pop('learning_rate', 1e-3)\n",
    "    lr_decay = kwargs.pop('lr_decay', 1.0)\n",
    "    batch_size = kwargs.pop('batch_size', 100)\n",
    "    num_epochs = kwargs.pop('num_epochs', 10)\n",
    "    num_train_samples = kwargs.pop('num_train_samples', 1000)\n",
    "    num_val_samples = kwargs.pop('num_val_samples', None)\n",
    "    print_every = kwargs.pop('print_every', 10)   \n",
    "    verbose = kwargs.pop('verbose', True)\n",
    "    \n",
    "    epoch = 0\n",
    "    best_val_acc = 0\n",
    "    best_params = {}\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    \n",
    "    num_train = data['X_train'].shape[0]\n",
    "    iterations_per_epoch = max(num_train // batch_size, 1)\n",
    "    num_iterations = num_epochs * iterations_per_epoch\n",
    "    \n",
    "\n",
    "    \n",
    "    for t in range(num_iterations):\n",
    "        # Make a minibatch of training data\n",
    "        batch_mask = np.random.choice(num_train, batch_size)\n",
    "        X_batch = data['X_train'][batch_mask]\n",
    "        y_batch = data['y_train'][batch_mask]\n",
    "        \n",
    "        # Compute loss and gradient\n",
    "        loss, grads = model.forwards_backwards(X_batch, y_batch)\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Perform a parameter update\n",
    "        for p, w in model.params.items():\n",
    "            model.params[p] = w - grads[p]*learning_rate\n",
    "          \n",
    "        # Print training loss\n",
    "        if verbose and t % print_every == 0:\n",
    "            print('(Iteration %d / %d) loss: %f' % (\n",
    "                   t + 1, num_iterations, loss_history[-1]))\n",
    "         \n",
    "        # At the end of every epoch, increment the epoch counter and decay\n",
    "        # the learning rate.\n",
    "        epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "        if epoch_end:\n",
    "            epoch += 1\n",
    "            learning_rate *= lr_decay\n",
    "        \n",
    "        # Check train and val accuracy on the first iteration, the last\n",
    "        # iteration, and at the end of each epoch.\n",
    "        first_it = (t == 0)\n",
    "        last_it = (t == num_iterations - 1)\n",
    "        if first_it or last_it or epoch_end:\n",
    "            train_acc = test_network(model, data['X_train'], data['y_train'],\n",
    "                num_samples= num_train_samples)\n",
    "            val_acc = test_network(model, data['X_val'], data['y_val'],\n",
    "                num_samples=num_val_samples)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "\n",
    "            if verbose:\n",
    "                print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
    "                       epoch, num_epochs, train_acc, val_acc))\n",
    "\n",
    "            # Keep track of the best model\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_params = {}\n",
    "                for k, v in model.params.items():\n",
    "                    best_params[k] = v.copy()\n",
    "        \n",
    "    model.params = best_params\n",
    "        \n",
    "    return model, train_acc_history, val_acc_history\n",
    "        \n",
    "\n",
    "# load data\n",
    "data = load_cifar10() \n",
    "train_data = { k: data[k] for k in ['X_train', 'y_train', \n",
    "                                    'X_val', 'y_val']}\n",
    "#######################################################################\n",
    "# TODO: Set up model hyperparameters                                  #\n",
    "#######################################################################\n",
    "\n",
    "# initialize model\n",
    "model = SoftmaxClassifier(hidden_dim = 900, weight_scale=1e-2)\n",
    "\n",
    "# start training    \n",
    "model, train_acc_history, val_acc_history = train_network(\n",
    "    model, train_data, learning_rate = 0.001,\n",
    "    lr_decay= 1, num_epochs=10, \n",
    "    batch_size= 64, print_every=1000)\n",
    "#######################################################################\n",
    "#                         END OF YOUR CODE                            #\n",
    "#######################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fcovGmpXvXXa"
   },
   "source": [
    "# 4.2(c) Report Accuracy\n",
    "\n",
    "Run the given code and report the accuracy on test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FwCq8pBhu6dz",
    "outputId": "34d20c85-d590-4804-a0ce-a69c42ed1502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.4955\n"
     ]
    }
   ],
   "source": [
    "# report test accuracy\n",
    "acc = test_network(model, data['X_test'], data['y_test'])\n",
    "print(\"Test accuracy: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTrmbULS7i2N"
   },
   "source": [
    "# 4.2(d) Plot\n",
    "\n",
    "Using the train_acc_history and val_acc_history, plot the train & val accuracy versus epochs on one plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "SPjtnbya9S7g",
    "outputId": "589cda0a-663a-4ed8-83ff-4b04c60f8edf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f21bce64400>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEEJ4CQl0IYTwEhLoQgjhJSTQhRDCS/x/17TcQAgrtSoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_acc_history,label=\"train accuracy\")\n",
    "plt.plot(val_acc_history,label=\"val accuracy\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of EECS 504 PS4: Backpropagation",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07bdddedb4ff4979ac8528ca64fedef9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "634f9f36e3234760a8b6232c057eb84b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d4d0dbbd0db49ddadc4561f6e5f3188",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_ddbd8ffe4d6d4ab29ad5aed91c0c1c63",
      "value": "170500096it [00:04, 40222129.72it/s]"
     }
    },
    "75890ba2430a4066add67456e197a51c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_07bdddedb4ff4979ac8528ca64fedef9",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c423ef0139c7460bb1aa6c5e6e19956e",
      "value": 1
     }
    },
    "8d4d0dbbd0db49ddadc4561f6e5f3188": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad34ee2892d94575b52b07b62e23d836": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_75890ba2430a4066add67456e197a51c",
       "IPY_MODEL_634f9f36e3234760a8b6232c057eb84b"
      ],
      "layout": "IPY_MODEL_c7fdbf1467c247f5b7b4bb58fc705c40"
     }
    },
    "c423ef0139c7460bb1aa6c5e6e19956e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c7fdbf1467c247f5b7b4bb58fc705c40": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ddbd8ffe4d6d4ab29ad5aed91c0c1c63": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
